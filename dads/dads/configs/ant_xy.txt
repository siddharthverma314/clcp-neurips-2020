# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

### TRAINING HYPERPARAMETERS -------------------
--run_train=1

# metadata flags
--save_model=dads
--save_freq=50
--record_freq=100
--vid_name=skill

# optimization hyperparmaters
--replay_buffer_capacity=10000

# (set clear_buffer_every_iter=1 for on-policy optimization)
--clear_buffer_every_iter=0
--initial_collect_steps=2000
--collect_steps=500
--num_epochs=1000

# skill dynamics optimization hyperparameters
--skill_dyn_train_steps=8
--skill_dynamics_lr=3e-4
--skill_dyn_batch_size=256

# agent hyperparameters
--agent_gamma=0.99
--agent_lr=3e-4
--agent_entropy=0.1
--agent_train_steps=64
--agent_batch_size=256

# (optional, do not change for on-policy) relabelling or off-policy corrections
--skill_dynamics_relabel_type=importance_sampling
--num_samples_for_relabelling=1
--is_clip_eps=10.

# (optional) skills can be resampled within the episodes, relative to max_env_steps
--min_steps_before_resample=2000
--resample_prob=0.02

# (optional) configure skill dynamics training samples to be only from the current policy
--train_skill_dynamics_on_policy=0

### SHARED HYPERPARAMETERS ---------------------
--environment=Ant-v4
--max_env_steps=200
--reduced_observation=2

# define the type of skills being learnt
--num_skills=10
--skill_type=discrete_uniform
--random_skills=100
--num_evals=10

# (optional) policy, critic and skill dynamics
--hidden_layer_size=512

# (optional) skill dynamics hyperparameters
--graph_type=default
--num_components=4
--fix_variance=1
--normalize_data=1

# (optional) clip sampled actions
--action_clipping=1.

# (optional) debugging
--debug=0

### EVALUATION HYPERPARAMETERS -----------------
--run_eval=0

# MPC hyperparameters
--planning_horizon=1
--primitive_horizon=10
--num_candidate_sequences=50
--refine_steps=10
--mppi_gamma=10
--prior_type=normal
--smoothing_beta=0.9
--top_primitives=5


### (optional) ENVIRONMENT SPECIFIC HYPERPARAMETERS --------
# DKitty hyperparameters
--expose_last_action=1
--expose_upright=1
--robot_noise_ratio=0.0
--root_noise_ratio=0.0
--upright_threshold=0.95
--scale_root_position=1
--randomize_hfield=0.0

# DKitty/DClaw
--observation_omission_size=0

# Cube Manipulation hyperparameters
--randomized_initial_distribution=1
--horizontal_wrist_constraint=0.3
--vertical_wrist_constraint=1.0
