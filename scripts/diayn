#!/usr/bin/env python3

from adversarial.env import DiaynWrapper, VideoWrapper
from adversarial.diayn import DiscreteDiayn, ContinuousDiayn
from adversarial.env import make_env
from adversarial.algo import SAC
from pyrl.actor import TanhGaussianActor
from pyrl.critic import DoubleQCritic
from pyrl.replay_buffer import ReplayBuffer
from pyrl.utils import collate
from pyrl.sampler import BaseSampler
from pyrl.logger import logger, Hyperparams
import argparse
from skvideo.io import vwrite
import ant_hrl_maze
import copy

import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt


###################
# PARSE ARGUMENTS #
###################

parser = argparse.ArgumentParser()

# sample params
parser.add_argument("--env-name", type=str, default="Ant-v4")
parser.add_argument("--eval-env-name", type=str, default="Ant-v4")
parser.add_argument("--path-length", type=int, default=200)

# general params
parser.add_argument("--device", type=str, default="cuda")
parser.add_argument("--hidden-dim", type=str, default="256,256")

# diayn params
parser.add_argument("--continuous", action="store_true", default=False)
parser.add_argument("--num-skills", type=int, default=10)
parser.add_argument("--reward-weight", type=int, default=5)
parser.add_argument("--xy-prior", action="store_true", default=False)

# sac params
parser.add_argument("--init-temperature", type=float, default=1)

# train params
parser.add_argument("--sample-repeats", type=int, default=10)
parser.add_argument("--train-repeats", type=int, default=300)
parser.add_argument("--eval-repeats", type=int, default=5)

# logging params
parser.add_argument("--logdir", type=str, default="/store/vsiddharth/adversarial/temp")

args = parser.parse_args()
args.hidden_dim = [int(d) for d in args.hidden_dim.split(",")]

env = make_env(args.env_name, args.device)
eval_env = make_env(args.eval_env_name, args.device)

if args.continuous:
    diayn_class = ContinuousDiayn
else:
    diayn_class = DiscreteDiayn
diayn = diayn_class(
    env.observation_space,
    args.hidden_dim,
    args.num_skills,
    args.reward_weight,
    _device=args.device,
    _truncate=2 if args.xy_prior else None,
)
eval_diayn = copy.deepcopy(diayn)
eval_diayn.model = diayn.model

env = DiaynWrapper(env, diayn)
eval_env = DiaynWrapper(eval_env, eval_diayn)

act = TanhGaussianActor(env.observation_space, env.action_space, args.hidden_dim)
crt = DoubleQCritic(env.observation_space, env.action_space, args.hidden_dim)
algo = SAC(
    actor=act,
    critic=crt,
    _device=args.device,
    _act_dim=0,  # not needed because we are not learning temperature
    _learnable_temperature=False,
    _init_temperature=args.init_temperature,
)

sampler = BaseSampler(act.action, _path_length=args.path_length, env=env)
rb = ReplayBuffer(env.observation_space, env.action_space, int(1e6), 256, args.device)


#################
# SETUP LOGGING #
#################

logger.initialize(
    {
        "sac": algo,
        "sampler": sampler,
        "replay_buffer": rb,
        "diayn": diayn,
        "diayn_eval": eval_diayn,
        "args": Hyperparams(vars(args)),
    },
    args.logdir,
)
logger.log_hyperparameters()
(logger.logdir / "videos").mkdir()
(logger.logdir / "plots").mkdir()


def write_video_and_plot(i, j):
    new_env = VideoWrapper(eval_env)
    vs = BaseSampler(
        lambda x: act.action(x, True), _path_length=args.path_length, env=new_env
    )
    if isinstance(diayn, DiscreteDiayn):
        with diayn.with_z(j):
            _, batch = vs.sample()
    else:
        _, batch = vs.sample()
    path = logger.logdir / "videos" / f"video_{i}_{j}.mp4"
    logger.log("Saving video to path {}".format(path))
    vwrite(path, new_env.get_video_and_clear())

    obs = batch["obs"]["observations"].detach().cpu().numpy()
    plt.plot(obs[:, 0].squeeze(), obs[:, 1].squeeze())


##########
# TRAIN! #
##########

for i in range(1000):
    for _ in range(args.sample_repeats):
        _, batch = sampler.sample()
        rb.add(batch)

    for j in range(args.train_repeats):
        # train sac
        batch = rb.sample()
        batch["rew"] = diayn.calc_rewards(batch["obs"])
        algo.update(batch, j)

        # train diayn
        batch = rb.sample()
        diayn.train(batch["obs"])

    # eval diayn
    obss = []
    for _ in range(args.eval_repeats):
        _, batch = sampler.sample()
        obss.append(batch["obs"])
    obss = collate(obss)
    eval_diayn.calc_rewards(obss)
    eval_diayn.train(obss)
    logger.epoch(i)

    # make plot and video
    if (i + 1) % 50 == 0:
        plt.cla()
        if not args.continuous:
            for j in range(args.num_skills):
                write_video_and_plot(i, j)
        else:
            for j in range(args.num_skills * 5):
                write_video_and_plot(i, j)
        plt.xlim(-5, 5)
        plt.ylim(-5, 5)
        plt.savefig(logger.logdir / "plots" / f"plot_{i}.png")
