#!/usr/bin/env python3

from adversarial.env import DiaynWrapper, VideoWrapper
from adversarial.diayn import DiscreteDiayn, ContinuousDiayn
from adversarial.sampler import AdversarialSampler
from adversarial.env import make_env
from adversarial.algo import SAC
from pyrl.actor import TanhGaussianActor
from pyrl.critic import DoubleQCritic
from pyrl.replay_buffer import ReplayBuffer
from pyrl.utils import collate, Flatten, torchify
from PIL import Image, ImageDraw
from pyrl.sampler import BaseSampler
from pyrl.logger import logger, Hyperparams, LogItems
import argparse
from skvideo.io import vwrite
import ant_hrl_maze
import numpy as np
import torch
import copy

import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt


###################
# PARSE ARGUMENTS #
###################

parser = argparse.ArgumentParser()

# sample params
parser.add_argument("--env-name", type=str, default="Ant-v4")
parser.add_argument("--eval-env-name", type=str, default="Ant-v4")
parser.add_argument("--path-length", type=int, default=200)
parser.add_argument("--threshold", type=float, default=-1)

# general params
parser.add_argument("--device", type=str, default="cuda")
parser.add_argument("--hidden-dim", type=str, default="256,256")
parser.add_argument("--batch-size", type=int, default=256)

# diayn params
parser.add_argument("--continuous", action="store_true", default=False)
parser.add_argument("--num-skills", type=int, default=20)
parser.add_argument("--reward-weight", type=float, default=2)
parser.add_argument("--xy-prior", action="store_true", default=False)

# sac params
parser.add_argument("--init-temperature", type=float, default=1)

# train params
parser.add_argument("--num-samples", type=int, default=int(1e6))
parser.add_argument("--train-per-sample", type=int, default=1)
parser.add_argument("--eval-repeats", type=int, default=5)
parser.add_argument("--eval-offset", type=int, default=1000)
parser.add_argument("--save-video-offset", type=int, default=50000)

# logging params
parser.add_argument("--logdir", type=str, required=True)

args = parser.parse_args()
args.hidden_dim = [int(d) for d in args.hidden_dim.split(",")]

orig_env = make_env(args.env_name, args.device)
orig_eval_env = make_env(args.eval_env_name, args.device)


#########
# ALICE #
#########

if args.continuous:
    diayn_class = ContinuousDiayn
else:
    diayn_class = DiscreteDiayn
alice_diayn = diayn_class(
    orig_env.observation_space,
    args.hidden_dim,
    args.num_skills,
    args.reward_weight,
    _device=args.device,
    _truncate=2 if args.xy_prior else None,
)
env = DiaynWrapper(orig_env, alice_diayn)
eval_env = DiaynWrapper(orig_eval_env, alice_diayn)

alice_eval_diayn = copy.deepcopy(alice_diayn)
alice_eval_diayn.model = alice_diayn.model
alice_act = TanhGaussianActor(
    env.observation_space, env.action_space, args.hidden_dim
)
alice_crt = DoubleQCritic(env.observation_space, env.action_space, args.hidden_dim)
alice_algo = SAC(
    actor=alice_act,
    critic=alice_crt,
    _device=args.device,
    _act_dim=0,  # not needed because we are not learning temperature
    _learnable_temperature=False,
    _init_temperature=args.init_temperature,
)
alice_rb = ReplayBuffer(
    env.observation_space, env.action_space, int(1e6), args.batch_size, args.device
)

#######
# BOB #
#######

bob_act = TanhGaussianActor(
    orig_env.observation_space,
    orig_env.action_space,
    args.hidden_dim,
)
bob_crt = DoubleQCritic(
    orig_env.observation_space, orig_env.action_space, args.hidden_dim
)
bob_algo = SAC(
    actor=bob_act,
    critic=bob_crt,
    _device=args.device,
    _act_dim=Flatten(orig_env.action_space).dim,
    _learnable_temperature=True,
    _init_temperature=args.init_temperature,
)
bob_rb = ReplayBuffer(
    orig_env.observation_space,
    orig_env.action_space,
    int(1e6),
    args.batch_size,
    args.device,
)


#################
# SETUP LOGGING #
#################

log = LogItems()
logger.initialize(
    {
        "alice": alice_algo,
        "bob": bob_algo,
        "alice_replay_buffer": alice_rb,
        "bob_replay_buffer": bob_rb,
        "alice_diayn": alice_diayn,
        "args": Hyperparams(vars(args)),
        "log": log,
    },
    args.logdir,
)
logger.log_hyperparameters()
(logger.logdir / "videos").mkdir()
(logger.logdir / "plots").mkdir()


def write_adv_video_and_plot(i, j):
    a_env = VideoWrapper(eval_env)
    b_env = VideoWrapper(eval_env.env)
    alice_sampler = BaseSampler(
        lambda x: alice_act.action(x, True), _path_length=args.path_length, env=a_env
    )
    bob_sampler = BaseSampler(
        lambda x: bob_act.action(x, True), _path_length=args.path_length, env=b_env
    )
    sampler = AdversarialSampler(alice_sampler, bob_sampler, 1)
    if isinstance(alice_diayn, DiscreteDiayn):
        with alice_diayn.with_z(j):
            alice_batch, bob_batch = sampler.sample()
    else:
        alice_batch, bob_batch = sampler.sample()
    path = logger.logdir / "videos" / f"adversarial_{i}_{j}.mp4"
    logger.log("Saving video to path {}".format(path))

    def make_banner(text, banner_width=30):
        banner = Image.new("RGB", (a_env.video[0].shape[1], banner_width))
        banner_draw = ImageDraw.Draw(banner)
        banner_draw.text((0, 0), text)
        return np.array(banner)

    alice_banner = make_banner("Alice")
    bob_banner = make_banner("Bob")
    banner = np.column_stack((alice_banner, bob_banner))

    alice_vid = np.array(a_env.video)
    bob_vid = np.array(b_env.video)
    alice_repeat = alice_vid[-1][None, ...].repeat(len(bob_vid), axis=0)
    bob_repeat = bob_vid[0][None, ...].repeat(len(alice_vid), axis=0)
    alice_vid = np.concatenate(
        (
            banner[None, ...].repeat(len(alice_vid), axis=0),
            np.concatenate((alice_vid, bob_repeat), axis=2),
        ),
        axis=1,
    )
    bob_vid = np.concatenate(
        (
            banner[None, ...].repeat(len(bob_vid), axis=0),
            np.concatenate((alice_repeat, bob_vid), axis=2),
        ),
        axis=1,
    )
    frames = np.concatenate((alice_vid, bob_vid), axis=0)
    vwrite(path, frames)

    alice_obs = alice_batch["obs"]["observations"].detach().cpu().numpy()
    bob_obs = bob_batch["obs"]["observations"].detach().cpu().numpy()
    plt.plot(alice_obs[:, 0].squeeze(), alice_obs[:, 1].squeeze())
    plt.plot(bob_obs[:, 0].squeeze(), bob_obs[:, 1].squeeze(), "grey")


##########
# TRAIN! #
##########

alice_turn = True
turn_count = 0
obs = None

# logging stuff
alice_steps = 0
bob_steps = 0

for i in range(args.num_samples + 1):
    # reset environment if needed
    if obs is None:
        obs = env.reset()

    # sample one sample
    if alice_turn:
        alice_steps += 1
        action = alice_act.action(obs)
    else:
        bob_steps += 1
        action = bob_act.action({k: v for k, v in obs.items() if k != "diayn"})
    next_obs, rew, done, info = env.step(action)
    if alice_turn:
        log["alice_info"] = torchify(info)
    else:
        log["bob_info"] = torchify(info)

    # add to replay buffer
    if alice_turn:
        rew.fill_(0)
        alice_rb.add(
            {"obs": obs, "next_obs": next_obs, "act": action, "done": done, "rew": rew}
        )
    else:
        bob_rb.add(
            {"obs": obs, "next_obs": next_obs, "act": action, "done": done, "rew": rew}
        )

    # increment observation
    obs = next_obs

    # train
    if alice_turn:
        for _ in range(args.train_per_sample):
            # train sac
            alice_batch = alice_rb.sample()
            with torch.no_grad():
                tmp = {k: v for k, v in alice_batch["obs"].items() if k != "diayn"}
                act = bob_act.action(tmp)
                val = bob_crt.forward(tmp, act)
                val = val.sign() * val.abs().log1p()
            alice_batch["rew"] = -val
            alice_batch["rew"] += alice_diayn.calc_rewards(alice_batch["obs"])
            alice_algo.update(alice_batch, turn_count)

            # train diayn
            alice_batch = alice_rb.sample()
            alice_diayn.train(alice_batch["obs"])
    else:
        for _ in range(args.train_per_sample):
            # train sac
            bob_batch = bob_rb.sample()
            bob_algo.update(bob_batch, turn_count)

    # switch turn and reset if needed
    turn_count += 1
    if turn_count > args.path_length:
        distance = obs["observations"][:2].norm()
        if alice_turn:
            turn_count = 0
            alice_turn = False
        elif args.threshold > 0 and distance < args.threshold:
            obs = None
            turn_count = 0
            alice_turn = True
        elif args.threshold <= 0 and turn_count > args.path_length:
            obs = None
            turn_count = 0
            alice_turn = True

    if i % args.eval_offset == 0:
        # eval diayn
        obss = []
        alice_sampler = BaseSampler(
            alice_act.action, _path_length=args.path_length, env=eval_env
        )
        for _ in range(args.eval_repeats):
            _, batch = alice_sampler.sample()
            obss.append(batch["obs"])
        obss = collate(obss)
        alice_eval_diayn.calc_rewards(obss)
        alice_eval_diayn.train(obss)

        # make metrics
        log["alice_steps"] = torch.tensor(alice_steps)
        log["bob_steps"] = torch.tensor(bob_steps)
        log["bob_turn_fraction"] = torch.tensor(bob_steps / (alice_steps + bob_steps))
        alice_steps = 0
        bob_steps = 0

        logger.epoch()

    if i % args.save_video_offset == 0:
        # adversarial video
        plt.cla()
        if not args.continuous:
            for j in range(args.num_skills):
                write_adv_video_and_plot(i, j)
        else:
            for j in range(args.num_skills * 5):
                write_adv_video_and_plot(i, j)
        plt.xlim(-5, 5)
        plt.ylim(-5, 5)
        plt.savefig(logger.logdir / "plots" / f"adversarial_{logger._epoch_num}.png")
        logger.log_snapshot()
